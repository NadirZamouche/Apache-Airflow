1. Create a "requirements.txt" file inside project foler and type your package with version for example: scikit-learn==1.7.2
Note: requirements.txt file can only have python packages nothing else like java etc.

2. Create a "Dockerfile" inside project folder and type (make sure it's the same version of Airflow as in compose.yaml file):
FROM apache/airflow:3.1.6
COPY requirements.txt /requirements.txt
RUN pip install --no-cache-dir -r /requirements.txt

3. Project cmd type: docker build . --tag extending_airflow:latest

4. Change the airflow image name in docker-compose.yaml from:
   image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:3.1.6}
   to:
   image: ${AIRFLOW_IMAGE_NAME:-extending_airflow:latest}

5. Create the DAG logic.

6. then in cmd once again: docker-compose up -d --no-deps --build airflow-apiserver airflow-scheduler
   note: in past versions of airflow there was airflow-webserver and then it has been replaced with airflow-apiserver,
   so always check by going to docker-compose.yaml file and see the different services.

and everytime u add a package to dependencies aka requirements.txt, create a dag to check and type these 3 comands:
docker build . --tag extending_airflow:latest
docker-compose up -d --no-deps --build airflow-apiserver airflow-scheduler
docker-compose up -d

--------------------------------------------------------------------------
Example:
1. I have added pyspark==3.5.1 to requirements.txt
2. create a task in DAG to check
3. typed these 3 comands (extended the docker image):
docker build . --tag extending_airflow:latest
docker-compose up -d --no-deps --build airflow-apiserver airflow-scheduler
docker-compose up -d